---
title: Predictive Analytics (2)
author: Brett Petch
date: 2020-03-05 09:30:00
layout: category-post
categories: 
    - week-9
    - Lecture
---

So far: The use of predictive analytics in crime and what it is, the different methods that are used to measure and predict. We also looked at who the people are that are likely to commit a crime, etc. When we talked about class: what if the data is coded a specific way, if you were to overpolice an area, then use this data, you'd still be overpolicing the same areas. 

Today: What are some of the implications of big data, justice, and inequality -- do technologies have ideologies embedded within them? When we look at different apps, what technologies are being used, what are they enabling, etc.

### Amazon: Neighbours app, connected to the Amazon Ring; the ring enables people to see crimes as they happen, etc.

<iframe style="width: 55vw; height: 45vh" src="https://www.youtube-nocookie.com/embed/F44O9hVuUVc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Connecting this video to 911. Alot of SNL style things; people way too happy, etc.

Who our neighbours and who aren't their neighbours. A very white, stereotypical...

### Case 2: [Citizen](https://citizen.com/). realtime crime info

### Case 3: Nextdoor: 

All of these have alot of mysterious things, etc. people will do racial profile

<iframe style="width: 55vw; height: 45vh" src="https://www.youtube-nocookie.com/embed/AOMPD0CqYxA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

BBQ Becky: bbqing in the park, said they were doing something illegal. So, we will go back to one of the concepts, in relation to these technologies, and reproducing social biases such as SketchFactor, are they racist? are they sexist? is it the way they're used by people or is it things that are implicit in their design.

## Recall Techno-fundamentalism, 
inerrant faith in the system, page rank or anything else can be used as a solution to problems. Algorithms, Apps, Programs will solve many if not all human problems. Is there something embedded within these technologies that code social inequalities and ideologies in their design. Are there certain ways in which a technology, like an application have ideologies embedded within them.

limiting people from using them: bridges with no sidewalks, parkbenches with dividers in them, etc

## "Discriminatory Design" (Ruha Benjamin)
Discriminatory design, What are the ways that technologies have ideologies embedded within them. Suggests that it reinforces existing social inequities, rather than address social inequalities. Building on this, she argues that there are technofixes, very close to techno-fundamentalism, wherein there is a coded bias in these technolgoies in what they offer us. They often claim to be race neutral, yet are encoded with biases inside of them.

Contra claims of technological solutionism, new technologies have particular social inequalities encoded into their design. For example, one of the things that is noteable about these apps is that instead of looking at what is causing the crime and inequality, who are the different problems. Social norms, racial norms, and social structures shape the way...

Many apps and technolgoies contain "engineered inequalities" in their design that enhance social inequaltieis.

Things to consider: what kinds of questions and concerns do they enable? What do they omit? How do they frame the issue? Who are the targets of new technologies?

Connecting this back to our regularly scheduled program:
Brainscans in prison: how do the brains lend themselves to social problems, using data from people already in jail. Trying to discover who are the criminals, we're not talking about wall street, government figures, etc. 

Can we think about these same types of questions in the way of predictive policing?

What kinds of questions and concerns does it enable? What does it omit? How does it frame the issue? Who are the targets?

WMDs like predictive policing are not designed to collect on everything, but rather things that are most-fixable. What are these different issues?

One of the things I want to look at is can we reverse some of this information -- there are some kinds of things like the [White Collar Crime Risk Zones](https://whitecollar.thenewinquiry.com/#dr5recg) What are some of the ways it challenges the "discriminatory design" of predicting policing?

With a partner:
what kinds of questions and concerns does it enable? What does it omit? How does it frame the issue? Who are your targets?
How does it frame the issue, who are the targets? 

